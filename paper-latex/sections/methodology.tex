In this section, we detail the methods and approaches we will be using in our research. 
The methodology is structured as follows:

\begin{enumerate}
    \item \textbf{Benchmarking}
    \begin{enumerate}
        \item Design
        \item Performance
        \item Worst Case Analysis
        \item Efficiency
    \end{enumerate}
    \item \textbf{Security Assumptions}
    \item \textbf{Validator Costing}
\end{enumerate}

% ==========================================================
% I.1 BENCHMARKING
% ==========================================================
\subsection{\textbf{Benchmarking}}

% ----------------------------------------------------------
% 1.1.1 DESIGN
% ----------------------------------------------------------
\subsubsection{\textbf{Design}}
For benchmarking design a code module will be developed to evaluate the performance of all the data availability solutions under the study. 
\vspace{1em}

% Polkadot
\paragraph{Polkadot}
For benchmarking of polkadot's ELVES we are using \textbf{Polkadot's mainnet}. The current test code implements:
\begin{itemize}
    \item System remarks for submission of data.
    \item Measuring the transaction fees
    \item Time taken for finalization of the block.
\end{itemize}
[We are also planning to use polkadot's on-chain telemetry (telemetry.polkadot.io) and logs for benchmarking.]
\vspace{1em}

% Celestia
\paragraph{Celestia}
For benchmarking Celestia's data availability solution, we utilize their \textbf{RPC} to submit and retrieve data blobs from the DA layer with their namespaces.

[We will also use the \textbf{Cosmos SDK} to interact with Celestia nodes and perform various operations such as data submission, retrieval, and verification.]
\vspace{1em}

% Espresso
\paragraph{Espresso's Tiramisu}
For Espresso's Tiramisu, we utilized their three-layered data availability architecture through their \textbf{API} endpoints. As of now, our testing framework implements:
\begin{itemize}
    \item Data availability metrics through \textbf{API} endpoints (query.main.net.espresso.network)
    \item Block analysis including TPS calculations and size measurements
\end{itemize}
[We are also planning to set up a local testnet node of Espresso and use their SDKs for more in-depth benchmarking.]
\vspace{1em}

% NEAR
\paragraph{NEAR}
For benchmarking NEAR's sharded data availability, we use \textbf{NEAR's} \textbf{mainnet} environments. 
Our benchmarking setup uses near-api and a custom smart contract deployed with near-sdk, which allows storing and retrieving base64 encoded data blobs on-chain. 
We record the transaction time, retrieval latency, and cost per MB to compare NEAR’s blob storage performance with other DA systems.
\vspace{1em}

% Avail
\paragraph{Avail}
For Avail's benchmarking we have currently utilized avails JavaScript \textbf{SDK} and Python implementations. The code framework includes:
\begin{itemize}
    \item Block bloat testing with automatic payload size adjustment
    \item Telemetry probing for network metrics
    \item Data submission through \textbf{AppID}-based categorization
    \item Block retrieval and verification mechanisms
\end{itemize}


\vspace{2em}

% ----------------------------------------------------------
% 1.1.2 PERFORMANCE
% ----------------------------------------------------------
\subsubsection{\textbf{Performance}}
\vspace{1em}

\paragraph{\textbf{Throughput Analysis}}
This section will analyze the maximum throughput capabilities of different DA solutions under various network conditions/data sizes.
\begin{itemize}
    \item \textbf{Test Parameters:} Time taken, data sizes
    \item \textbf{Metrics:} MB/s
\end{itemize}

\begin{table}[h!] \centering \caption{Comparative Throughput Analysis of DA Solutions} \label{tab:throughput_vertical} \setlength{\tabcolsep}{5pt} % reduce column padding 
    \renewcommand{\arraystretch}{1.1} % tighten row spacing 
    \begin{tabular}{|l|c|c|c|} \hline \textbf{Protocol} & \textbf{Throughput (MB/s)} \\ 
        \hline Polkadot & X \\ 
        Celestia & X \\ 
        Espresso & X \\ 
        NEAR & X \\ 
        Avail & X \\ \hline 
    \end{tabular}
 \end{table}

\vspace{1em}


\paragraph{\textbf{TPS}}
This section analyzes the transactions per second (TPS) capabilities of different DA solutions.
\begin{itemize}
    \item \textbf{Test Parameters:} Network conditions, data sizes, transactions count
    \item \textbf{Metrics:} TPS
\end{itemize}

\begin{table}[h!] \centering \caption{Comparative TPS Analysis of DA Solutions} \label{tab:throughput_vertical} \setlength{\tabcolsep}{5pt} % reduce column padding 
    \renewcommand{\arraystretch}{1.1} % tighten row spacing 
    \begin{tabular}{|l|c|c|c|} \hline \textbf{Protocol} & \textbf{TPS} \\ 
        \hline Polkadot & X \\ 
        Celestia & X \\ 
        Espresso & X \\ 
        NEAR & X \\ 
        Avail & X \\ \hline 
    \end{tabular}
 \end{table}

\vspace{3em}


\paragraph{\textbf{Cost per MB}}
This section analyzes the cost per megabyte (MB) of data processed by different DA solutions.
\begin{itemize}
    \item \textbf{Test Parameters:} Data sizes, Cost in native token conversion to USD
    \item \textbf{Metrics:} \$/MB
\end{itemize}

% (table here...)

\begin{table}[h!] \centering \caption{Comparative \$/MB Analysis of DA Solutions} \label{tab:throughput_vertical} \setlength{\tabcolsep}{5pt} % reduce column padding 
    \renewcommand{\arraystretch}{1.1} % tighten row spacing 
    \begin{tabular}{|l|c|c|c|} \hline \textbf{Protocol} & \textbf{Cost (\$/MB)} \\ 
        \hline Polkadot & X \\ 
        Celestia & X \\ 
        Espresso & X \\ 
        NEAR & X \\ 
        Avail & X \\ \hline 
    \end{tabular}
 \end{table}

\vspace{1em}


\paragraph{\textbf{Latency}}
This section will analyze the maximum time taken by the DA solution to finalize a block.
\begin{itemize}
    \item \textbf{Test Parameters:} Network conditions, data sizes, node count
    \item \textbf{Metrics:} seconds (s)
\end{itemize}

% (table here...)'
\begin{table}[h!] \centering \caption{Comparative Latency Analysis of DA Solutions} \label{tab:throughput_vertical} \setlength{\tabcolsep}{5pt} % reduce column padding 
    \renewcommand{\arraystretch}{1.1} % tighten row spacing 
    \begin{tabular}{|l|c|c|c|} \hline \textbf{Protocol} & \textbf{Latency (s)} \\ 
        \hline Polkadot & X \\ 
        Celestia & X \\ 
        Espresso & X \\ 
        NEAR & X \\ 
        Avail & X \\ \hline 
    \end{tabular}
 \end{table}
\vspace{1em}


\paragraph{\textbf{Max Block Size}}
This section analyzes the maximum block size capabilities of different DA solutions.
\begin{itemize}
    \item \textbf{Test Parameters:} Data sizes.
    \item \textbf{Metrics:} Block size (MB)
\end{itemize}

% (table here...)
\begin{table}[h!] \centering \caption{Comparative Block Size Analysis of DA Solutions} \label{tab:throughput_vertical} \setlength{\tabcolsep}{5pt} % reduce column padding 
    \renewcommand{\arraystretch}{1.1} % tighten row spacing 
    \begin{tabular}{|l|c|c|c|} \hline \textbf{Protocol} & \textbf{Block Size (MB)} \\ 
        \hline Polkadot & X \\ 
        Celestia & X \\ 
        Espresso & X \\ 
        NEAR & X \\ 
        Avail & X \\ \hline 
    \end{tabular}
 \end{table}

\vspace{1em}



\paragraph{\textbf{Data Retrieval}}
This section analyzes the maximum data retrieval time capabilities of different DA solutions i.e. the time taken to retrieve a specific size/piece of data from the network.
\begin{itemize}
    \item \textbf{Test Parameters:} Network conditions, data sizes.
    \item \textbf{Metrics:} Retrieval time (s)
\end{itemize}

% (table here...)
\begin{table}[h!] \centering \caption{Data Retrieval Latency Analysis of DA Solutions} \label{tab:throughput_vertical} \setlength{\tabcolsep}{5pt} % reduce column padding 
    \renewcommand{\arraystretch}{1.1} % tighten row spacing 
    \begin{tabular}{|l|c|c|c|} \hline \textbf{Protocol} & \textbf{ Retrieval Latency (s)} \\ 
        \hline Polkadot & X \\ 
        Celestia & X \\ 
        Espresso & X \\ 
        NEAR & X \\ 
        Avail & X \\ \hline 
    \end{tabular}
 \end{table}

\vspace{1em}
% ----------------------------------------------------------
% 1.1.3 WORST CASE SCENARIO
% ----------------------------------------------------------
\subsubsection{\textbf{Worst Case Analysis}}
In this section, we will evaluate the worst-case performance scenarios for each DA solution. 
This includes analyzing how each solution handles extreme conditions such as high network latency, large data loads, and potential attacks or failures within the network.


\vspace{1em}

\paragraph{\textbf{Cost per MB (Inclusion Phase)}}
This sub-subsection evaluates the cost per megabyte (MB) incurred during worst-case data inclusion scenarios for each DA solution. 
We measure how gas fee surges, and payload sizes affect the overall inclusion cost.

\begin{itemize}
    \item \textbf{Test Parameters:}  Data size (1KB–1MB), gas/fee volatility
    \item \textbf{Metrics:} Cost per MB (\$/MB)
\end{itemize}

\begin{table}[h!]
\centering
\caption{Worst-Case Cost per MB During Data Inclusion}
\label{tab:worstcase_cost_inclusion}
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|l|c|}
    \hline
    \textbf{Protocol} & \textbf{Worst-Case Cost (\$/MB)} \\
    \hline
    Polkadot & X \\
    Celestia & X \\
    Espresso & X \\
    NEAR & X \\
    Avail & X \\
    \hline
\end{tabular}
\end{table}

\vspace{2em}

\paragraph{\textbf{Latency (Worst Case)}}
This measures the maximum inclusion time under stress conditions (e.g., full block utilization).
\begin{itemize}
    \item \textbf{Test Parameters:} Network latency, data size, node count
    \item \textbf{Metrics:} Latency (seconds)
    \item \textbf{Finality Impact:} Evaluates how consensus finality duration contributes to overall latency, including delayed confirmations or resistance under high-load conditions.
\end{itemize}

\begin{table}[h!]
\centering
\caption{Worst-Case Data Latency}
\label{tab:worstcase_inclusion_latency}
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|l|c|}
    \hline
    \textbf{Protocol} & \textbf{Latency (s)} \\
    \hline
    Polkadot & X \\
    Celestia & X \\
    Espresso & X \\
    NEAR & X \\
    Avail & X \\
    \hline
\end{tabular}
\end{table}

\vspace{3em}


\paragraph{\textbf{Data Retrieval (Worst Case)}}
We measure the time required to fetch data blobs under adverse conditions.
\begin{itemize}
    \item \textbf{Test Parameters:} Network latency, node failures, retrieval requests per second
    \item \textbf{Metrics:} Data Retrieval Time (s)
    \item \textbf{Node Failure Impact:} Evaluates retrieval degradation under partial or complete node unavailability, including re-routing latency and redundancy recovery performance.
\end{itemize}

\begin{table}[h!]
\centering
\caption{Worst-Case Data Retrieval Latency}
\label{tab:worstcase_retrieval_latency}
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|l|c|}
    \hline
    \textbf{Protocol} & \textbf{Retrieval Latency (s)} \\
    \hline
    Polkadot & X \\
    Celestia & X \\
    Espresso & X \\
    NEAR & X \\
    Avail & X \\
    \hline
\end{tabular}
\end{table}


\vspace{5em}

% ----------------------------------------------------------
% 1.1.4 EFFICIENCY Add dummy box plot graph here
% ----------------------------------------------------------
\subsubsection{\textbf{Efficiency}}
This section focuses on comparative efficiency based on best and worst metrics of throughput, TPS, and cost per MB results. 
Efficiency metrics are derived from normalized performance ratios under controlled network environments.

\vspace{2em}

\paragraph{\textbf{Data}}
This subsection evaluates data transmission and Latency efficiency across DA (Data Availability) solutions.  
It compares bandwidth utilization, encoding overhead, and redundancy management under variable network conditions.  
Key considerations include:
\vspace{1em}

\begin{itemize}
    \item \textbf{Latency:} Reflects the overall efficiency of data flow, capturing how quickly transactions move from submission to availability. Lower latency directly translates to higher throughput and better real-time performance across DA layers.
\end{itemize}

These metrics provide a foundation for comparing the raw data handling capability of each DA layer independent of consensus or proof generation complexity.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth]{./figures/output.png}
    \caption{Latency efficiency comparison across DA solutions}
    \label{fig:efficiency_boxplot}
\end{figure}

\vspace{2em}

% ===================== NEW SUBSECTION =====================
\paragraph{\textbf{Storage}}
This subsection focuses on storage efficiency and persistence trade-offs once data becomes part of the network.  
It captures how network-level encoding, redundancy schemes, and node distribution affect the overall data footprint and storage cost.

Key considerations include:
\vspace{1em}

\begin{itemize}
    \item \textbf{ Data Size (Post-Inclusion):} Analyzes how the effective data size increases once added to the network due to encoding, sharding, and availability sampling. This metric quantifies real storage growth compared to raw payload size, reflecting true on-network storage overhead.
\end{itemize}

\begin{table}[h!]
\centering
\caption{Data Storage and Transfer Metrics Across DA Solutions}
\label{tab:data_efficiency_metrics}
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|l|c|c|c|}
    \hline
    \textbf{Protocol} & \textbf{Data Size} \\
    \hline
    Polkadot & X \\
    Celestia & X \\
    Espresso & X \\
    NEAR & X \\
    Avail & X \\
    \hline
\end{tabular}
\end{table}

\vspace{2em}

% ===================== PROOF SUBSECTION =====================
\paragraph{\textbf{Proof}}
This subsection focuses on efficiency aspects related to proof generation, verification, and proof size optimization.  
Proof-related efficiency directly impacts network scalability and client-side verification costs.  
Key focus points include:
\begin{itemize}
    \item \textbf{Proof Size:} Analysis of proof compactness and its relation to network bandwidth and storage.
    \item \textbf{Proof Computation:} Benchmarks proof generation latency and verification complexity.
\end{itemize}

\begin{table}[h!]
\centering
\caption{Proof Size and Computation Metrics Across DA Solutions}
\label{tab:proof_efficiency_metrics}
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|l|c|c|}
    \hline
    \textbf{Protocol} & \textbf{Proof Size (kB)} & \textbf{Proof Computation (ms)} \\
    \hline
    Polkadot & X & X \\
    Celestia & X & X \\
    Espresso & X & X \\
    NEAR & X & X \\
    Avail & X & X \\
    \hline
\end{tabular}
\end{table}

This evaluation highlights the trade-offs between computational efficiency and proof soundness within different DA architectures.

\vspace{1em}


% ==========================================================
% I.2 SECURITY ASSUMPTIONS
% ==========================================================
\subsection{\textbf{Security Assumptions}}
In this section, we outline the security assumptions made during the evaluation of the DA solutions. 
These assumptions are critical for understanding the context in which the solutions are being assessed and the potential vulnerabilities that may arise.


\begin{table}[ht]
\centering
\caption{Security Assumptions of Major DA Solutions}
\setlength{\tabcolsep}{3pt}  % Reduced padding
\renewcommand{\arraystretch}{1.25}
\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|p{1.5cm}|}
\hline
\textbf{Network} & 
\textbf{Honest Ratio} & 
\textbf{Validator Count} & 
\textbf{And More...} \\
\hline
Polkadot & 
$\geq \frac{2}{3}$ & 
600 &
\\
\hline
Celestia & 
$\geq 1$ & 
100 &
 \\
\hline
Espresso & 
$> \frac{1}{2}$ &
100 &
 \\
\hline
NEAR & 
$\geq \frac{2}{3}$ &
500 &
\\
\hline
Avail & 
$\geq 1$ & 
105 &
\\
\hline
\end{tabular}
\label{tab:da-security-compact}
\end{table}

\vspace{5em}


% ==========================================================
% I.3 VALIDATOR COSTING
% ==========================================================
\subsection{\textbf{Validator Costing}}
In this section, we will analyze the costs incurred by validators in maintaining and operating each DA solution. 
This includes hardware costs, bandwidth costs, and any other operational expenses associated with running a validator node.

\begin{table*}[ht]
\centering
\caption{Minimum or Recommended Hardware Requirements for DA/Validator Nodes 
(Data sourced from network documentation: \cite{Polkadot_Validator_Requirements}, \cite{Celestia_Nodes_Overview}, \cite{Espresso_Mainnet_Node}, \cite{NEAR_Validator_Hardware}, \cite{Avail_FullNode_Overview})}
\setlength{\tabcolsep}{8pt}  % Increased spacing between columns
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\textbf{Network} & \textbf{CPU} & \textbf{RAM} & \textbf{Storage} & \textbf{Bandwidth} & \textbf{Entry Fees} & \textbf{Cost}\\ 
\hline
Polkadot & 8 cores & 32\,GB & 2\,TB NVMe & 500\,Mbps & \$XXX & \$XXX \\ 
Celestia & 8 cores & 64\,GB & 8\,TB NVMe & 1\,Gbps & \$XXX & \$XXX \\ 
Espresso & 4 cores & 8\,GB & 1\,TB SSD & 100\,Mbps & \$XXX & \$XXX \\ 
NEAR & 8 cores & 32\,GB & 3\,TB NVMe & - & \$XXX & \$XXX \\ 
Avail & 4 cores & 8\,GB & 20\,GB SSD & 50\,Mbps & \$XXX & \$XXX \\
\hline
\end{tabular}
\label{tab:da-node-reqs}
\end{table*}
