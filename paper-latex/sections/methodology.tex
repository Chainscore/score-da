In this section, we detail the methods and approaches we will be using in our research. 
The methodology is structured as follows:

\begin{enumerate}
    \item \textbf{Benchmarking}
    \begin{enumerate}
        \item Design
        \item Performance
        \item Worst Case Analysis
        \item Efficiency
    \end{enumerate}
    \item \textbf{Security Assumptions}
    \item \textbf{Validator Costing}
\end{enumerate}

% ==========================================================
% I.1 BENCHMARKING
% ==========================================================
\subsection{\textbf{Benchmarking}}

% ----------------------------------------------------------
% 1.1.1 DESIGN
% ----------------------------------------------------------
\subsubsection{\textbf{Design}}
For benchmarking design a code module will be developed to evaluate the performance of all the data availability solutions under the study. 
\vspace{1em}

% Polkadot
\paragraph{Polkadot}
For benchmarking of polkadot's ELVES we are using \textbf{Polkadot's mainnet}. The current test code implements:
\begin{itemize}
    \item System remarks for submission of data.
    \item Measuring the transaction fees
    \item Time taken for finalization of the block.
\end{itemize}
[We are also planning to use polkadot's on-chain telemetry (telemetry.polkadot.io) and logs for benchmarking.]
\vspace{1em}

% Celestia
\paragraph{Celestia}
For benchmarking Celestia's data availability solution, we utilize their \textbf{RPC} to submit and retrieve data blobs from the DA layer with their namespaces.

[We will also use the \textbf{Cosmos SDK} to interact with Celestia nodes and perform various operations such as data submission, retrieval, and verification.]
\vspace{1em}

% Espresso
\paragraph{Espresso's Tiramisu}
For Espresso's Tiramisu, we utilized their three-layered data availability architecture through their \textbf{API} endpoints. As of now, our testing framework implements:
\begin{itemize}
    \item Data availability metrics through \textbf{API} endpoints (query.main.net.espresso.network)
    \item Block analysis including TPS calculations and size measurements
\end{itemize}
[We are also planning to set up a local testnet node of Espresso and use their SDKs for more in-depth benchmarking.]
\vspace{1em}

% NEAR
\paragraph{NEAR}
For benchmarking NEAR's sharded data availability, we use \textbf{NEAR's testnet} and \textbf{mainnet} environments. 
Our benchmarking setup uses near-api and a custom smart contract deployed with near-sdk, which allows storing and retrieving base64 encoded data blobs on-chain. 
We record the transaction time, retrieval latency, and cost per MB to compare NEARâ€™s blob storage performance with other DA systems.
\vspace{1em}

% Avail
\paragraph{Avail}
For Avail's benchmarking we have currently utilized avails JavaScript \textbf{SDK} and Python implementations. The code framework includes:
\begin{itemize}
    \item Block bloat testing with automatic payload size adjustment
    \item Telemetry probing for network metrics
    \item Data submission through \textbf{AppID}-based categorization
    \item Block retrieval and verification mechanisms
\end{itemize}


\vspace{2em}

% ----------------------------------------------------------
% 1.1.2 PERFORMANCE
% ----------------------------------------------------------
\subsubsection{\textbf{Performance}}
\vspace{1em}

\paragraph{\textbf{Throughput Analysis}}
This section will analyze the maximum throughput capabilities of different DA solutions under various network conditions/data sizes.
\begin{itemize}
    \item \textbf{Test Parameters:} Time taken, data sizes
    \item \textbf{Metrics:} MB/s
    \item \textbf{Environment:} Simulated network with controlled conditions
\end{itemize}

\begin{table}[h!] \centering \caption{Comparative Throughput Analysis of DA Solutions} \label{tab:throughput_vertical} \setlength{\tabcolsep}{5pt} % reduce column padding 
    \renewcommand{\arraystretch}{1.1} % tighten row spacing 
    \begin{tabular}{|l|c|c|c|} \hline \textbf{Protocol} & \textbf{Throughput (MB/s)} \\ 
        \hline Polkadot & X \\ 
        Celestia & X \\ 
        Espresso & X \\ 
        NEAR & X \\ 
        Avail & X \\ \hline 
    \end{tabular}
 \end{table}

\vspace{1em}


\paragraph{\textbf{TPS}}
This section analyzes the transactions per second (TPS) capabilities of different DA solutions.
\begin{itemize}
    \item \textbf{Test Parameters:} Network conditions, data sizes, node count
    \item \textbf{Metrics:} TPS
    \item \textbf{Environment:} Simulated network with controlled conditions
\end{itemize}

\begin{table}[h!] \centering \caption{Comparative TPS Analysis of DA Solutions} \label{tab:throughput_vertical} \setlength{\tabcolsep}{5pt} % reduce column padding 
    \renewcommand{\arraystretch}{1.1} % tighten row spacing 
    \begin{tabular}{|l|c|c|c|} \hline \textbf{Protocol} & \textbf{TPS} \\ 
        \hline Polkadot & X \\ 
        Celestia & X \\ 
        Espresso & X \\ 
        NEAR & X \\ 
        Avail & X \\ \hline 
    \end{tabular}
 \end{table}

\vspace{3em}


\paragraph{\textbf{Cost per MB}}
This section analyzes the cost per megabyte (MB) of data processed by different DA solutions.
\begin{itemize}
    \item \textbf{Test Parameters:} Data sizes, Cost in native token conversion to USD
    \item \textbf{Metrics:} \$/MB
    \item \textbf{Environment:} Simulated network with controlled conditions
\end{itemize}

% (table here...)

\begin{table}[h!] \centering \caption{Comparative \$/MB Analysis of DA Solutions} \label{tab:throughput_vertical} \setlength{\tabcolsep}{5pt} % reduce column padding 
    \renewcommand{\arraystretch}{1.1} % tighten row spacing 
    \begin{tabular}{|l|c|c|c|} \hline \textbf{Protocol} & \textbf{Cost (\$/MB)} \\ 
        \hline Polkadot & X \\ 
        Celestia & X \\ 
        Espresso & X \\ 
        NEAR & X \\ 
        Avail & X \\ \hline 
    \end{tabular}
 \end{table}

\vspace{1em}


\paragraph{\textbf{Latency}}
This section will analyze the maximum time taken by the DA solution to finalize a block.
\begin{itemize}
    \item \textbf{Test Parameters:} Network conditions, data sizes, node count
    \item \textbf{Metrics:} seconds (s)
    \item \textbf{Environment:} Simulated network with controlled conditions
\end{itemize}

% (table here...)'
\begin{table}[h!] \centering \caption{Comparative Latency Analysis of DA Solutions} \label{tab:throughput_vertical} \setlength{\tabcolsep}{5pt} % reduce column padding 
    \renewcommand{\arraystretch}{1.1} % tighten row spacing 
    \begin{tabular}{|l|c|c|c|} \hline \textbf{Protocol} & \textbf{Latency (s)} \\ 
        \hline Polkadot & X \\ 
        Celestia & X \\ 
        Espresso & X \\ 
        NEAR & X \\ 
        Avail & X \\ \hline 
    \end{tabular}
 \end{table}
\vspace{1em}


\paragraph{\textbf{Max Block Size}}
This section analyzes the maximum block size capabilities of different DA solutions.
\begin{itemize}
    \item \textbf{Test Parameters:} Network conditions, data sizes, node count
    \item \textbf{Metrics:} Block size (MB)
    \item \textbf{Environment:} Simulated network with controlled conditions
\end{itemize}

% (table here...)
\begin{table}[h!] \centering \caption{Comparative Block Size Analysis of DA Solutions} \label{tab:throughput_vertical} \setlength{\tabcolsep}{5pt} % reduce column padding 
    \renewcommand{\arraystretch}{1.1} % tighten row spacing 
    \begin{tabular}{|l|c|c|c|} \hline \textbf{Protocol} & \textbf{Block Size (MB)} \\ 
        \hline Polkadot & X \\ 
        Celestia & X \\ 
        Espresso & X \\ 
        NEAR & X \\ 
        Avail & X \\ \hline 
    \end{tabular}
 \end{table}

\vspace{1em}


% ----------------------------------------------------------
% 1.1.3 WORST CASE SCENARIO
% ----------------------------------------------------------
\subsubsection{\textbf{Worst Case Analysis}}
In this section, we will evaluate the worst-case performance scenarios for each DA solution. 
This includes analyzing how each solution handles extreme conditions such as high network latency, large data loads, and potential attacks or failures within the network.

\vspace{5em}

% ----------------------------------------------------------
% 1.1.4 EFFICIENCY Add dummy box plot graph here
% ----------------------------------------------------------
\subsubsection{\textbf{Efficiency}}
This section focuses on comparative efficiency based on best and worst metrics of throughput, TPS, and cost per MB results. 
Efficiency metrics are derived from normalized performance ratios under controlled network environments.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth]{./figures/output.png}
    \caption{Dummy efficiency comparison across DA solutions}
    \label{fig:efficiency_boxplot}
\end{figure}

\vspace{1em}

% ==========================================================
% I.2 SECURITY ASSUMPTIONS
% ==========================================================
\subsection{\textbf{Security Assumptions}}
In this section, we outline the security assumptions made during the evaluation of the DA solutions. 
These assumptions are critical for understanding the context in which the solutions are being assessed and the potential vulnerabilities that may arise.


\begin{table}[ht]
\centering
\caption{Security Assumptions of Major Data Availability Solutions}
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.25}
\begin{tabular}{lp{6.2cm}}
\hline
\textbf{Network} & \textbf{Security Assumption Summary} \\ 
\hline
\textbf{Polkadot (ELVES)} & Requires $\geq 2/3$ honest validators. Data availability is guaranteed through deterministic erasure-coded chunk distribution and validator bitfield votes; light clients rely on validator honesty (no DAS). \\[2pt]
\textbf{Celestia} & Secure if at least one honest full node serves samples. Uses probabilistic Data Availability Sampling (DAS) with fraud proofs to detect withheld or invalid erasure-coded data. \\[2pt]
\textbf{Espresso (Tiramisu)} & Assumes majority-honest committee or fallback network. Combines fast optimistic path (committee-based) with verifiable fallback via cryptographic proofs (VID). \\[2pt]
\textbf{NEAR (Blob Store)} & Relies on $\geq 2/3$ honest validators under Nightshade consensus. Provides inclusion proofs but no erasure coding or sampling; trust tied to validator set. \\[2pt]
\textbf{Avail} & Secure if $\geq 1$ honest node performs sampling. Uses KZG polynomial commitments with DAS and fraud proofs, offering light-client verifiability similar to Celestia. \\ 
\hline
\end{tabular}
\label{tab:da-security-compact}
\end{table}


\vspace{1em}

% ==========================================================
% I.3 VALIDATOR COSTING
% ==========================================================
\subsection{\textbf{Validator Costing}}
In this section, we will analyze the costs incurred by validators in maintaining and operating each DA solution. 
This includes hardware costs, bandwidth costs, and any other operational expenses associated with running a validator node.

\begin{table*}[ht]
\centering
\caption{Minimum or Recommended Hardware Requirements for DA/Validator Nodes}
\setlength{\tabcolsep}{8pt}  % Increased spacing between columns
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Network} & \textbf{CPU} & \textbf{RAM} & \textbf{Storage} & \textbf{Bandwidth} & \textbf{Cost} \\ 
\hline
Polkadot & 8 cores & 32\,GB & 2\,TB NVMe & 500\,Mbps & \$XXX/mo \\ 
Celestia & 8 cores & 64\,GB & 8\,TB NVMe & 1\,Gbps & \$XXX/mo \\ 
Espresso & 4 cores & 8\,GB & 1\,TB SSD & 100\,Mbps & \$XXX/mo \\ 
NEAR & 8 cores & 32--48\,GB & 3--4\,TB NVMe & - & \$XXX/mo \\ 
Avail & 4 cores & 8\,GB & 20--40\,GB SSD & 50\,Mbps & \$XXX/mo \\
\hline
\end{tabular}
\label{tab:da-node-reqs}
\end{table*}

